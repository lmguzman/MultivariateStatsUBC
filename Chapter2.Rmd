---
title: "Ch2 Matrix Algebra: A Summary"
date: "October 15, 2015"
output: pdf_document
---

##2.1 The ecological data matrix##

In the field of ecology, the matrix is often a representation of what ecologists might commonly refer to as a table.
A matrix, like a table, contains Objects (x, or the 'rows' of a table) and Descriptors (y, or the 'columns' of a table), 
where $\{x | x \in N , x \leq n \}$ and $\{y | y \in N , y \leq p \}$
and any element of the matrix can be denoted $y_{ij}$
where $i$ denotes the row and $j$ denotes the column number.


For example, and Object might be a sampling site and the Descriptor would be species. That is, a study where species fluctuates over sampling sites.
However, species could also be and Object, and a descriptor might be a trait associated with said species. In this experiment species are sampled and we observe how traits fluctuate in response to said species. 

*In general, Objects are limited by the researcher a priori and descriptors are response variables encompassing an infinite set.* 

So, given a table of Objects (rows) and Descriptors (columns), matricies help to standardize and package such data sets, making them easier to manipulate and analyse. 


##2.2 Association matricies##

An association matrix measures the degree of association between elements of matrix (eg. $x_1$ and $x_2$). 

There are two ways of looking at associations:

Q mode - compares all possible pairs of **objects** in a matrix A.   
R mode - compares all possible pairs of **descriptors** in a matrix A. 

example: if one wanted to compare Objects $x_1$ and $x_2$ of a matrix A, they would essentially be comparing the two rows 
$x_{1} = [y_{11}, y_{12}, y_{13},...y_{1p}]$ and
$x_2 = [y_{21}, y_{22}, y_{23},...y_{2p}]$ 

and a matrix B can be defined to describe the association between $x_1$ and $x_2$. 



##2.3 Special matricies##

Certain matricies of special use and interest are:

1. Diagonal matrix where, given a square matrix ($n$ rows = $n$ columns), all non diagonal elements are zero. 
2. Identity matrix (I) is a diagonal matrix with all non-zero values being 1. Given some square matrix C, I Has the property: $I*C=C*I=C$
3. Triangle matrix where all elements above or below the diagonal of a square matrix are zero. 
4. Transpose matrix where given a matrix $D$ with dimensions $n x p$ can have its rows and columns flipped (transposed) to give a new matrix $D'$ with dimensions $p x n$ where the elements of D' can be described by $D'_{ij} = D_{ji}$.  


##2.4 Vectors and scaling##

A single column with $n$ rows can also be called a column vector. 
\[\begin{pmatrix}
1 \\
3 
\end{pmatrix}
\]

A single row with p columns can also be called a row vector. 
\[\begin{pmatrix}
1 & 3
\end{pmatrix}
\]
Vectors of two elements (ex [2,3]) can be represented in 2-D space as a line from (0,0) to a point (2,3). The length of said vector can be calculated geometrically and is called the 'magnitude' of the vector. 
Likewise vectors of 3 elements can be drawn in 3-D space, and in general n-dimensional vectors exist in n-space. 


##2.5 matrix addition and multiplication##

Recall that in ecology matricies will often represent sets of ecological data.
For example a matrix E might contain site-species (rows-columns) information for one month of sampling.
The following month another matrix F of site-species info is collected - what is the total number of species collected per site?
Total matrix = E + F where elements of E, $e_{ij}$, are added to their respective elements in F, $f_{ij}$. 

In extending this example, consider that the sampling techniques used at each site vary slightly. In this case, a correction factor might be applied to each site.
Instead of manually applying a correction factor to each row, one can instead build a correction matrix containing all correction factors and apply it to the matricies in question.
For example, Site 1 sampled in July might have found 3 species in abundances of 1, 5, and 35 represented by \[ S =\begin{pmatrix}
1 & 5 & 35
\end{pmatrix}
\].
To apply the correction factor, apply the matrix \[ C = \begin{pmatrix}
1 \\
2 \\
4
\end{pmatrix}
\]
Multiplying $S*C$ gives $(1x1)+(5x2)+(35x4)=151$ (the total number of species sampled in July, after correcting for sampling methods). 

*Multiplying matricies requires that the number of rows of on matrix match the number of columns of the other.*  
Also, $A*B$ is not necessarily the same as $B*A$ 
Multiplying matricies has numerous properties and uses, see text for extensive examples.

##2.6 Determinant##

The determinant of a matrix is a scalar number that 'determines' the factor by which a matrix scales in n-dimensions. 
In other words, if the rank (dimensions) of a matrix A is 2, and the determinant is say, 5, then if matrix A is used to transform a 2-D object (ex: a drawing of a cat), then all properties of that object will be expanded by a factor of 5. 

Determinant properties:   

1.  Is sensitive to all changes in the matrix(object) it is being applied to. i.e. if a matrix A is to be transformed to A' by the determinant $f(b)$, then $f(b)$ must recognize all changes in all elements of A.   
2.  If $A=A'$, then the determinant $f(b)=0$.   
3.  $f(b)$ is a scalar number.   

##2.7 Rank of a matrix##

As mentioned, the rank of the matrix is the dimensions of a matrix. However, the dimensions that a matrix occupies can't always be determined by simply counting the numbers of rows and columns.
That is, a n x m matrix doesn't always take up n or m dimensions. 

Example: matrix 

\[ A = \begin{pmatrix}
3 & 6 \\
2 & 4\\
\end{pmatrix}
\]  
where column 2 is simply a multiple of column 1. This means that matrix $A$ only occupies one dimension. 
however, if matrix \[ V = \begin{pmatrix}
3 & 7 \\
2 & 4\\
\end{pmatrix}
\]  

then the matrix V occupies two dimensions because the column vectors need two dimensions to be represented (this is more obvious when displayed graphically). One vector is not superimposed on the other. 

##2.8 Matrix inversion##

Similar to algebra, where the inverse of a variable a is denoted $a^-1$, the inverse of a matrix A is denoted $A^-1$. 
Furthermore where $a*a^-1 = 1$, $A*A^-1 = I$ (identity matrix).

Inverse matricies are therefore very useful in matrix algebra. When the properties of a matrix A can't be directly measured, transforming the matrix A back to the identity matrix, and storing the transformations as $A^-1$ can be a useful technique in characterizing A.
This works because $A*A^-1 = I$ therefore $A \to I | A^-1$ 

##2.9 Eigenvalues and Eigenvectors##

These are often a source of confusion conceptually. 
First, it is important to remember that eigenvectors and eigenvalues are not "their own thing", they are a property of all square matricies.
Geometrically speaking, an eigenvector is a vector of matrix A that remains unchanged in it's orientation when a non-zero transformation is applied to A.
However, in this transformation of A the eigenvector may expand or contract, this is measured by the eigenvalue, which is a number not a vector. 

Eigenvalues are important in population projection models to determine if a population with grow or shrink over time. 
Similarly, the eigenvector of your dataset is that property of your matrix that may shrink or grow, but whose direction remains unchanged, for example, the proportion of adults to juveniles. 

For any square matrix, it follows that:

$AV = \lambda V$

Where V is the eigenvector and $\lambda$ is the eigenvalue. 
Every point on the same line as an eigenvector is an eigenspaec, and each is associated with an eigenvalue. Both eigenspaces depend on both columns of A. 

If you keep multiplying a matrix A by any vector V, you get a sequence $V$, $AV$, $A^2V$, $A^3V$ ...
Eigenspaces attract that sequence. 

##2.10 Singular value decomposition##

This technique is useful for 'decomposing' difficult matricies into several smaller, easier to handle ones.

For example, a matrix A with $m x n$ dimensions can be decomposed into:

$A(m x n) = A' (m x k) * A''( k x k) * A ''' (k x n)$ 



